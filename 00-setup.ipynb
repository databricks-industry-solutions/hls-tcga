{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TCGA Project Setup - Configuration Hub\n",
    "\n",
    "This notebook is the **single source of truth** for project configuration.\n",
    "\n",
    "## What this notebook does:\n",
    "1. Collects configuration parameters from user input\n",
    "2. Creates/updates the main `config.json` file\n",
    "3. Creates Unity Catalog resources (catalog, schema, volume)\n",
    "4. Validates the setup\n",
    "\n",
    "All downstream components (ETL pipelines, jobs, analysis notebooks) will read from the generated `config.json`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Configure Your Environment\n",
    "\n",
    "Provide the following parameters for your TCGA project:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Create widgets for user input\n",
    "dbutils.widgets.text(\"catalog_name\", \"kermany\", \"Unity Catalog Name\")\n",
    "dbutils.widgets.text(\"schema_name\", \"tcga\", \"Schema Name\")\n",
    "dbutils.widgets.text(\"volume_name\", \"tcga_files\", \"Volume Name\")\n",
    "\n",
    "# Get values from widgets\n",
    "catalog = dbutils.widgets.get(\"catalog_name\")\n",
    "schema = dbutils.widgets.get(\"schema_name\")\n",
    "volume = dbutils.widgets.get(\"volume_name\")\n",
    "\n",
    "# Validate inputs\n",
    "if not catalog or catalog == \"<CHANGE TO YOUR CATALOG NAME>\":\n",
    "    raise ValueError(\"Please provide a valid catalog name\")\n",
    "if not schema:\n",
    "    raise ValueError(\"Please provide a valid schema name\")\n",
    "if not volume:\n",
    "    raise ValueError(\"Please provide a valid volume name\")\n",
    "\n",
    "# Display configuration\n",
    "print(\"=\"*60)\n",
    "print(\"TCGA Project Configuration\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Catalog: {catalog}\")\n",
    "print(f\"Schema: {schema}\")\n",
    "print(f\"Volume: {volume}\")\n",
    "print(f\"Volume Path: /Volumes/{catalog}/{schema}/{volume}\")\n",
    "print(f\"Database: {catalog}.{schema}\")\n",
    "print(\"=\"*60)"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Generate Configuration File\n",
    "\n",
    "Create the main `config.json` that will be used by all components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "import json\n",
    "import os\n",
    "\n",
    "# Define configuration structure\n",
    "config = {\n",
    "    \"lakehouse\": {\n",
    "        \"catalog\": catalog,\n",
    "        \"schema\": schema,\n",
    "        \"volume\": volume\n",
    "    },\n",
    "    \"api_paths\": {\n",
    "        \"cases_endpt\": \"https://api.gdc.cancer.gov/cases\",\n",
    "        \"files_endpt\": \"https://api.gdc.cancer.gov/files\",\n",
    "        \"data_endpt\": \"https://api.gdc.cancer.gov/data/\"\n",
    "    },\n",
    "    \"pipeline\": {\n",
    "        \"max_workers\": 64,\n",
    "        \"max_records\": 20000,\n",
    "        \"force_download\": False,\n",
    "        \"retry_attempts\": 3,\n",
    "        \"timeout_seconds\": 300\n",
    "    }\n",
    "}\n",
    "\n",
    "# Write configuration to file\n",
    "config_path = os.path.abspath('./config.json')\n",
    "with open(config_path, 'w') as f:\n",
    "    json.dump(config, f, indent=2)\n",
    "\n",
    "print(\"✓ Configuration file created/updated at:\", config_path)\n",
    "print(\"\\nConfiguration contents:\")\n",
    "print(json.dumps(config, indent=2))"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Create Unity Catalog Resources\n",
    "\n",
    "Create the catalog, schema, and volume if they don't already exist."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Note: Creating a catalog requires METASTORE_ADMIN privilege\n",
    "# Uncomment the line below if you have the required permissions\n",
    "# spark.sql(f\"CREATE CATALOG IF NOT EXISTS {catalog}\")\n",
    "\n",
    "print(\"Creating Unity Catalog resources...\\n\")\n",
    "\n",
    "# Create schema\n",
    "try:\n",
    "    spark.sql(f\"CREATE SCHEMA IF NOT EXISTS {catalog}.{schema}\")\n",
    "    print(f\"✓ Schema {catalog}.{schema} created or already exists\")\n",
    "except Exception as e:\n",
    "    print(f\"⚠ Warning: Could not create schema. Error: {str(e)}\")\n",
    "    print(f\"  Make sure catalog '{catalog}' exists and you have permissions.\")\n",
    "    raise\n",
    "\n",
    "# Create volume  \n",
    "try:\n",
    "    spark.sql(f\"CREATE VOLUME IF NOT EXISTS {catalog}.{schema}.{volume}\")\n",
    "    print(f\"✓ Volume {catalog}.{schema}.{volume} created or already exists\")\n",
    "except Exception as e:\n",
    "    print(f\"⚠ Warning: Could not create volume. Error: {str(e)}\")\n",
    "    raise\n",
    "\n",
    "print(f\"\\n✓ Setup complete! Unity Catalog resources are ready.\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Verify Setup\n",
    "\n",
    "Verify that the resources were created successfully."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Verify schema exists\n",
    "print(\"Verifying resources...\\n\")\n",
    "\n",
    "try:\n",
    "    schemas_df = spark.sql(f\"SHOW SCHEMAS IN {catalog}\")\n",
    "    schema_exists = schemas_df.filter(f\"databaseName = '{schema}'\").count() > 0\n",
    "    \n",
    "    if schema_exists:\n",
    "        print(f\"✓ Schema {catalog}.{schema} verified\")\n",
    "    else:\n",
    "        print(f\"⚠ Schema {catalog}.{schema} not found\")\n",
    "except Exception as e:\n",
    "    print(f\"⚠ Could not verify schema: {str(e)}\")\n",
    "\n",
    "# Verify volume exists\n",
    "try:\n",
    "    volumes_df = spark.sql(f\"SHOW VOLUMES IN {catalog}.{schema}\")\n",
    "    volume_exists = volumes_df.filter(f\"volume_name = '{volume}'\").count() > 0\n",
    "    \n",
    "    if volume_exists:\n",
    "        print(f\"✓ Volume {catalog}.{schema}.{volume} verified\")\n",
    "        \n",
    "        # Get volume details\n",
    "        volume_path = f'/Volumes/{catalog}/{schema}/{volume}'\n",
    "        print(f\"  Volume path: {volume_path}\")\n",
    "    else:\n",
    "        print(f\"⚠ Volume {catalog}.{schema}.{volume} not found\")\n",
    "except Exception as e:\n",
    "    print(f\"⚠ Could not verify volume: {str(e)}\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Export Configuration for Other Notebooks\n",
    "\n",
    "Export configuration variables for use in other notebooks via `%run`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Export variables for notebooks that use %run ./00-setup\n",
    "volume_path = f'/Volumes/{catalog}/{schema}/{volume}'\n",
    "database_name = f'{catalog}.{schema}'\n",
    "\n",
    "# API endpoints\n",
    "cases_endpt = config['api_paths']['cases_endpt']\n",
    "files_endpt = config['api_paths']['files_endpt']\n",
    "data_endpt = config['api_paths']['data_endpt']\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Configuration variables exported:\")\n",
    "print(\"=\"*60)\n",
    "print(f\"catalog = '{catalog}'\")\n",
    "print(f\"schema = '{schema}'\")\n",
    "print(f\"volume = '{volume}'\")\n",
    "print(f\"volume_path = '{volume_path}'\")\n",
    "print(f\"database_name = '{database_name}'\")\n",
    "print(f\"cases_endpt = '{cases_endpt}'\")\n",
    "print(f\"files_endpt = '{files_endpt}'\")\n",
    "print(f\"data_endpt = '{data_endpt}'\")\n",
    "print(\"=\"*60)"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ✅ Setup Complete!\n",
    "\n",
    "### Next Steps:\n",
    "\n",
    "1. **Download Data**: Run `01-data-download.py` to fetch TCGA data\n",
    "2. **Deploy Pipeline**: Use Databricks Asset Bundle\n",
    "   ```bash\n",
    "   databricks bundle deploy --target dev\n",
    "   databricks bundle run tcga_data_workflow --target dev\n",
    "   ```\n",
    "3. **Run Analysis**: Execute `02-tcga-expression-clustering-optimized.py`\n",
    "\n",
    "### Configuration Location:\n",
    "- Main config: `./config.json`\n",
    "- All pipelines and jobs will read from this file"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
