# Job definitions for TCGA project
# Optimized for AWS with specific compute resources per workload

resources:
  jobs:
    # Complete TCGA data pipeline workflow
    tcga_data_workflow:
      name: "[${bundle.target}] TCGA Data Workflow"

      # Job clusters optimized for specific workloads
      job_clusters:
        # Single node cluster for data download (64 cores for concurrency)
        - job_cluster_key: "download_cluster"
          new_cluster:
            num_workers: 0  # Single node (driver-only)
            node_type_id: "${var.download_node_type}"  # r5d.16xlarge: 64 vCPUs, 512 GB RAM
            spark_version: "14.3.x-scala2.12"
            spark_conf:
              spark.databricks.cluster.profile: "singleNode"
              spark.master: "local[*]"
              spark.databricks.delta.preview.enabled: "true"
              spark.sql.adaptive.enabled: "true"
            custom_tags:
              ResourceClass: "SingleNode"
            data_security_mode: "USER_ISOLATION"

        # Memory-optimized cluster for ETL processing
        - job_cluster_key: "etl_cluster"
          new_cluster:
            autoscale:
              min_workers: 2
              max_workers: 8
            node_type_id: "${var.etl_node_type}"  # r5.2xlarge: 8 vCPUs, 64 GB RAM per node
            spark_version: "14.3.x-scala2.12"
            spark_conf:
              spark.databricks.delta.preview.enabled: "true"
              spark.databricks.delta.optimizeWrite.enabled: "true"
              spark.databricks.delta.autoCompact.enabled: "true"
              spark.sql.adaptive.enabled: "true"
              spark.sql.adaptive.coalescePartitions.enabled: "true"
            data_security_mode: "USER_ISOLATION"

        # Analysis cluster for ML workloads
        - job_cluster_key: "analysis_cluster"
          new_cluster:
            autoscale:
              min_workers: 2
              max_workers: 6
            node_type_id: "${var.analysis_node_type}"  # r5d.xlarge: 4 vCPUs, 32 GB RAM
            spark_version: "14.3.x-cpu-ml-scala2.12"
            spark_conf:
              spark.databricks.delta.preview.enabled: "true"
              spark.sql.adaptive.enabled: "true"
            data_security_mode: "USER_ISOLATION"

      # Job tasks
      tasks:
        # Task 1: Setup Unity Catalog resources
        - task_key: "setup"
          job_cluster_key: "download_cluster"
          notebook_task:
            notebook_path: ../00-setup.ipynb
            base_parameters:
              catalog_name: "${var.catalog_name}"
              schema_name: "${var.schema_name}"
              volume_name: "${var.volume_name}"
          timeout_seconds: 600

        # Task 2: Download data from GDC (uses single node with 64 cores)
        - task_key: "download_data"
          depends_on:
            - task_key: "setup"
          job_cluster_key: "download_cluster"
          notebook_task:
            notebook_path: ../01-data-download.py
            base_parameters:
              force_download: "false"
              max_workers: "${var.max_workers}"
              max_records: "20000"
          timeout_seconds: 14400  # 4 hours
          max_retries: 2
          retry_on_timeout: true

        # Task 3: Run DLT pipeline (uses its own cluster config from pipelines.yml)
        - task_key: "run_dlt_pipeline"
          depends_on:
            - task_key: "download_data"
          pipeline_task:
            pipeline_id: "${resources.pipelines.tcga_etl_pipeline.id}"
            full_refresh: false

        # Task 4: Run expression clustering analysis
        - task_key: "expression_clustering"
          depends_on:
            - task_key: "run_dlt_pipeline"
          job_cluster_key: "analysis_cluster"
          notebook_task:
            notebook_path: ../02-tcga-expression-clustering-optimized.py
            base_parameters:
              n_top_genes: "1000"
              n_pca_components: "50"
              use_sample: "false"
          timeout_seconds: 7200  # 2 hours

      # Schedule (daily at 2 AM) - starts paused
      schedule:
        quartz_cron_expression: "0 0 2 * * ?"
        timezone_id: "America/Los_Angeles"
        pause_status: "PAUSED"

      # Timeout for entire job
      timeout_seconds: 28800  # 8 hours

      # Max concurrent runs
      max_concurrent_runs: 1

      # Format
      format: MULTI_TASK

    # Incremental data refresh job (for periodic updates)
    tcga_incremental_refresh:
      name: "[${bundle.target}] TCGA Incremental Refresh"

      job_clusters:
        # Single node for incremental download
        - job_cluster_key: "refresh_download_cluster"
          new_cluster:
            num_workers: 0  # Single node
            node_type_id: "${var.download_node_type}"
            spark_version: "14.3.x-scala2.12"
            spark_conf:
              spark.databricks.cluster.profile: "singleNode"
              spark.master: "local[*]"
            custom_tags:
              ResourceClass: "SingleNode"
            data_security_mode: "USER_ISOLATION"

      tasks:
        # Download only new data
        - task_key: "download_new_data"
          job_cluster_key: "refresh_download_cluster"
          notebook_task:
            notebook_path: ../01-data-download.py
            base_parameters:
              force_download: "false"
              max_workers: "32"
              max_records: "5000"
          timeout_seconds: 3600

        # Incremental DLT pipeline refresh
        - task_key: "incremental_pipeline_refresh"
          depends_on:
            - task_key: "download_new_data"
          pipeline_task:
            pipeline_id: "${resources.pipelines.tcga_etl_pipeline.id}"
            full_refresh: false

      # Schedule (weekly on Sunday at 3 AM)
      schedule:
        quartz_cron_expression: "0 0 3 ? * SUN"
        timezone_id: "America/Los_Angeles"
        pause_status: "PAUSED"

      timeout_seconds: 7200  # 2 hours
      max_concurrent_runs: 1
      format: MULTI_TASK
